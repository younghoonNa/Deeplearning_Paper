![image](https://user-images.githubusercontent.com/38518648/225702505-e45b45b3-b3c3-4dc3-938e-d5b27056ffff.png)


## Problem
1. Our optimizer used SGD
    -  Gradient descent algorithm cost is very expensive, can easier fall into local minima
2. Each batch have different distribution. It can make Internal Covariant Shift
3. Internal Covariant Shift: change distribution of network activation due to the change in network parameters during training.
4. We used ReLU activate function , As a result, half of our activation is zero

## Main Idea
Main idea: Calculating mean(ğ) and variance(ğˆ^ğŸ) of training data in mini batch ğ’™. <br>
   Also training (ğœ¸, ğœ·) of each layer. And activation normalized zero-mean, unit variance ğ’™Â Ì‚.  <br>
   Activation output changed ğ’š=ğœ¸ğ’™Â Ì‚+ğœ·,  (ğœ¸=ğˆ,  ğœ·=ğ) 

## Related work
<b> Train deep ResNet to competitive accuracies without normalization </b>
1. Learnable scalar at the end of each residual branch(initialized to zero)
2. Normalizer-Free Resnet 
    - Residual branch at initialization and apply Scaled Weight Standardization 
    - Same acc of ResNet on ImaegNet but less then EfficientNet and not stable at large Batch size

## Process
1. Computation of Residual branch

$$ h_{(i+1)} = h_i + \alpha f_i (h_i/\beta_i) $$

ğ’‰_ğ’Š : input i-th residual block <br>
ğ’‡_ğ’Š : i-th function computed by i-th residual branch, preserve Var(ğ’‡_ğ’Š (ğ’›)) = Var(ğ’›) <br> 
ğœ¶ : rate of the variance of the activation after each residual block <br>
ğœ·_ğ’Š=âˆš(ğ‘½ğ’‚ğ’“(ğ’‰_ğ’Š ) )  <br>

â†’ Var(ğ’‰_(ğ’Š+ğŸ) )  = Var(ğ’‰_ğ’Š )+ğœ¶^ğŸ 

2. Scaled Weight Standardization

$$ \hat{W}_{ij} = (W_{ij} - \mu_i) / \sqrt{N}\sigma_i $$

ğ_ğ’Š : ğŸ/ğ‘µ  ğšº_ğ’‹    â†’  mean  <br>
ğˆ^ğŸ : ğŸ/ğ‘µ ğšº_ğ£  (ğ–_ğ¢ğ£  âˆ’ğ_ğ’Š )^ğŸ   â†’  standard deviation <br>

3. ReLU scaled by ğœ¸ that is non-linearity specific scaler
  - The combination of the ğ›„-scaled activate function and Scaled Weight Standardized layer ensure variance preserving 

$$ ReLU = \gamma = \sqrt{2/(1-(1/n))} $$

- When initialize hidden layer of Residual branch, our variance close to 1
- Input ğ’™ got linearity sampled ğ’©(ğŸ,ğŸ)
- ReLU output ğ’ˆ(ğ’™)= max(0, 1) will sampled from Gaussian distribution with variance ğˆ_ğ’ˆ^ğŸ=(ğŸ/ğŸ)(ğŸâˆ’(ğŸ/ğ…))
- Similarity of Kaiming He Weight initialization
++ Gradient Clipping method



